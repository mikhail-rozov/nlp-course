{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7264c6b3-6950-42f2-ae63-46e98a9117ba",
   "metadata": {},
   "source": [
    "## Практическое задание к уроку 6 по теме \"Классификация текста. Анализ тональности текста\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d1152-6f65-4b9e-87e3-882e30d5acdc",
   "metadata": {},
   "source": [
    "Взять ноутбук colab_text_classification_part1.ipynb который разбирали на занятии и\n",
    "добавить пункты которые мы пропустили  \n",
    "1. Посмотрите на токены если будут мусорные добавьте их в стоп слова и обучите\n",
    "заново  \n",
    "2. Проверьте изменилось ли качество при лемматизации/и без неё  \n",
    "3. Замените все токены которые принадлежат сущностям на их тег. Проверьте\n",
    "изменилось ли качество после этого "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2153e-c604-4054-9e3d-08dcd3f19945",
   "metadata": {},
   "source": [
    "Загрузим библиотеки и датасеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69728cff-c88c-4187-9567-c91bf3391b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8fcf27-3cd4-4f16-97f7-980a16a73e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../../Теория/Lesson_6/train.tsv\", delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(\"../../Теория/Lesson_6/test.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7df5593-2093-4117-a632-218e0cfc732e",
   "metadata": {},
   "source": [
    "1. Минимальная обработка."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e7bd2-90b0-45f2-adbc-43e8ba39925c",
   "metadata": {},
   "source": [
    "Сделаем минимальную предобработку, как на уроке -  \n",
    "удалим символы пропуска строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89302817-635d-471d-8a37-786819a743b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['review'] = train_df['review'].apply(lambda x: x.replace('<br />', ' '))\n",
    "test_df['review'] = test_df['review'].apply(lambda x: x.replace('<br />', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b03cd0-4880-4a3c-a1b0-6ed8b42c4565",
   "metadata": {},
   "source": [
    "Обучим модель на текущем состоянии датасета. В качестве  \n",
    "векторайзера возьмём CountVectorizer, т.к. он показал себя  \n",
    "лучше, чем Tfidf. Возьмём униграммы и биграммы слов и не  \n",
    "забудем отмасштабировать данные перед логистической регрессией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e956cb1-e02e-4d9d-9dab-f1047d771e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "scaler = MaxAbsScaler()\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = make_pipeline(vectorizer, scaler, classifier)\n",
    "model.fit(train_df['review'], train_df['is_positive']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8df4a-b5a1-4c7b-921a-3950fb521da1",
   "metadata": {},
   "source": [
    "Напишем функцию для оценки моделей, т.к. этот кусок кода  \n",
    "будем часто использовать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46e5908e-19cb-479d-a028-65cac7fa8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    pred = model.predict(test_df['review'])\n",
    "    return accuracy_score(test_df['is_positive'], pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e9080-5e38-4e7b-b697-c9d6dc09a94f",
   "metadata": {},
   "source": [
    "Будем записывать все результаты в словарь для последующей  \n",
    "визуализации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b714343-0001-4a6d-aff5-ed9bb459ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a16426-9878-4eb3-a204-0ed438ced5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90376\n"
     ]
    }
   ],
   "source": [
    "results['Minimum preprocessing'] = eval_model(model)\n",
    "print(results['Minimum preprocessing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06043b-bb47-4106-9404-708ae2947a5c",
   "metadata": {},
   "source": [
    "2. \"Мусорные\" токены."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2cc82-e41d-4f93-bfff-0fcecc6a37e1",
   "metadata": {},
   "source": [
    "Для обработки \"лишних\" токенов будем использовать аргумент  \n",
    "векторайзера `max_df`, который будет фильтровать наиболее  \n",
    "часто встречающиеся токены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc106a3-344c-489d-840c-f54f7604e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2), max_df=0.4)\n",
    "model = make_pipeline(vectorizer, scaler, classifier)\n",
    "model.fit(train_df['review'], train_df['is_positive']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b062e3a-7e29-4159-bc1c-d050baad6fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90544\n"
     ]
    }
   ],
   "source": [
    "results['Min + exclude stopwords'] = eval_model(model)\n",
    "print(results['Min + exclude stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1649d2-5922-4513-b107-b39d2fa20927",
   "metadata": {},
   "source": [
    "Результат стал немного лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b7cfb-c089-4424-ab1c-1579e770b482",
   "metadata": {},
   "source": [
    "3. Лемматизация."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4fe9f-3cab-4318-96cf-c457df2a9e0f",
   "metadata": {},
   "source": [
    "Для лемматизации и замены токенов на сущности в следующем  \n",
    "задании воспользуемся библиотекой spacy. Парсер использовать  \n",
    "не будем, т.к. текст у нас уже разбит на объекты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58fdbc6b-0785-45ed-9de8-930b3f416c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c838d8c9-3d70-40cb-bf9e-0bf0831ae797",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(train_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a252cf0-688a-450e-8beb-1dd4e6bfe0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 25000/25000 [05:32<00:00, 75.18it/s]\n"
     ]
    }
   ],
   "source": [
    "lemmatized = []\n",
    "for doc in tqdm(docs, total=len(train_df), position=0):\n",
    "     lemmatized.append(' '.join(token.lemma_ for token in doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb03fd63-c323-4c99-9127-13016667de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['review_lemma'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "262956ce-e096-412c-96a9-8b678e74e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87056\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_df['review_lemma'], train_df['is_positive'])\n",
    "results['Min + lemmatization'] = eval_model(model)\n",
    "print(results['Min + lemmatization'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72dad8-c4c0-4bbc-8765-3d6b21618b68",
   "metadata": {},
   "source": [
    "Результат получился значительно хуже. Дополнительно проверим  \n",
    "ещё результат работы, где векторайзер не фильтрует слова по  \n",
    "частоте. Вполне возможно, что после лемматизации часть важных  \n",
    "слов отфильтровалась, т.к. слова объединились и превысили порог."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2901ade6-5741-45b8-9799-7a62ad1419a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.max_df = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b13660e1-8c53-4b5a-a97d-f6feaa27220c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86772\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_df['review_lemma'], train_df['is_positive'])\n",
    "results['Min + stopwords + lemmatization'] = eval_model(model)\n",
    "print(results['Min + stopwords + lemmatization'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c08f51-15ab-4bae-8096-bf54d5f69d0b",
   "metadata": {},
   "source": [
    "Нет, без фильтрации токенов результат получился ещё хуже."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af8587-eaba-4278-8dcf-ef6e419d5919",
   "metadata": {},
   "source": [
    "4. Замена токенов на сущности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdb5eb29-d13a-41f1-bb63-5032eb5b2780",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(train_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9beb202-0eb6-46e1-9a33-13e0471ce68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 25000/25000 [05:31<00:00, 75.41it/s]\n"
     ]
    }
   ],
   "source": [
    "replaced = []\n",
    "for doc in tqdm(docs, total=len(train_df), position=0):\n",
    "    spam = []\n",
    "    for token in doc:\n",
    "        spam.append(token.ent_type_ if token.ent_type else token.text)\n",
    "    replaced.append(' '.join(spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "735d2404-5193-42ba-b205-e72d7980d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['review_replace'] = replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b48612-7163-4b0b-9892-1ac36f843e34",
   "metadata": {},
   "source": [
    "Здесь тоже проверим оба варианта: с фильтрацией  \n",
    "слов и без неё. Потому что сущности могут оказаться  \n",
    "слишком частыми токенами и будут отфильтрованы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf8950b7-da79-4e76-9573-e13519d77e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9022\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_df['review_replace'], train_df['is_positive'])\n",
    "results['Min + replace entities'] = eval_model(model)\n",
    "print(results['Min + replace entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02dda395-356e-4cd1-b243-4dd4c08ed123",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.max_df = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ab4a7ec-3923-4567-80df-f36b04820d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90252\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_df['review_replace'], train_df['is_positive'])\n",
    "results['Min + stopwords + entities'] = eval_model(model)\n",
    "print(results['Min + stopwords + entities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c246e6ca-e4a0-45ba-832e-46f9d266b56a",
   "metadata": {},
   "source": [
    "Сведём результаты в таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e34dbff1-93da-4f37-b917-88ddb4def82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Min + exclude stopwords</th>\n",
       "      <td>0.90544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minimum preprocessing</th>\n",
       "      <td>0.90376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min + stopwords + entities</th>\n",
       "      <td>0.90252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min + replace entities</th>\n",
       "      <td>0.90220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min + lemmatization</th>\n",
       "      <td>0.87056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min + stopwords + lemmatization</th>\n",
       "      <td>0.86772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 accuracy\n",
       "Min + exclude stopwords           0.90544\n",
       "Minimum preprocessing             0.90376\n",
       "Min + stopwords + entities        0.90252\n",
       "Min + replace entities            0.90220\n",
       "Min + lemmatization               0.87056\n",
       "Min + stopwords + lemmatization   0.86772"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results.values(), index=results.keys(), \n",
    "             columns=['accuracy']).sort_values('accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df07f4-97dc-4adc-94be-b2b6a29e6364",
   "metadata": {},
   "source": [
    "Лучший результат показала минимальная предобработка  \n",
    "с исключением часто встречающихся токенов. Лемматизация  \n",
    "оказала отрицательный эффект на метрику. Замена токенов  \n",
    "на тэги сущностей чуть ухудшила результат."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
