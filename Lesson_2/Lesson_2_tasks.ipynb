{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6d5fdf-f6fa-4c58-aac0-18a3fff49e24",
   "metadata": {},
   "source": [
    "## Практическое задание к уроку 2 по теме \"Создание признакового пространства\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4744e68b-53dc-4dbd-bc7d-71169b08e1dc",
   "metadata": {},
   "source": [
    "Продолжим обработку данных с Твиттера. \n",
    "\n",
    "1. Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "    • Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "    • Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "    • Исключим стоп-слова с помощью stop_words='english'.\n",
    "    • Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names().\n",
    "  \n",
    "2. Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "    • Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "    • Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "    • Исключим стоп-слова с помощью stop_words='english'.\n",
    "    • Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().  \n",
    "3. Натренируем gensim.models.Word2Vec модель на наших данных.\n",
    "    • Тренировать будем на токенизированных твитах combine_df['tweet_token']\n",
    "    • Установим следующие параметры: size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34.\n",
    "    • Используем функцию train() с параметром total_examples равным длине combine_df['tweet_token'], количество epochs установим 20.\n",
    "  \n",
    "4. Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово positive = \"dinner\", и модель вытащит из корпуса наиболее похожие слова c помощью функции most_similar. То же самое попробуем со словом \"trump\".  \n",
    "  \n",
    "5. Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова).\n",
    "Давайте проверим векторное представление любого слова из нашего корпуса, например \"food\".  \n",
    "\n",
    "6. Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите. Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. Теперь у нас есть 200 функций word2vec для наших данных.\n",
    "Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите. В цикле сделать:  vec += model_w2v[word].reshape((1, size))\n",
    "и поделить финальный вектор на количество слов в твите.\n",
    "На выходе должен получиться wordvec_df.shape = (49159, 200)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4de78d-e216-4a4b-a540-fc79ce6c28a9",
   "metadata": {},
   "source": [
    "Загрузим данные с предыдущего практического задания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c31d838-0e9a-4518-9b89-4085707b7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b84369-a352-4f7c-9004-edbd9ffb80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Lesson_1/data/preprocessed_data.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "570591f6-0e13-465a-b10e-f494f9eb16e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>model love you take with you all the time in ur</td>\n",
       "      <td>[model, love, you, take, with, you, all, the, ...</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              tweet  \\\n",
       "id                                                             \n",
       "1     0.0  when father is dysfunctional and is so selfish...   \n",
       "2     0.0  thanks for lyft credit cannot use cause they d...   \n",
       "3     0.0                                bihday your majesty   \n",
       "4     0.0    model love you take with you all the time in ur   \n",
       "5     0.0                  factsguide society now motivation   \n",
       "\n",
       "                                          tweet_token  \\\n",
       "id                                                      \n",
       "1   [when, father, is, dysfunctional, and, is, so,...   \n",
       "2   [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "3                             [bihday, your, majesty]   \n",
       "4   [model, love, you, take, with, you, all, the, ...   \n",
       "5              [factsguide, society, now, motivation]   \n",
       "\n",
       "                                 tweet_token_filtered  \\\n",
       "id                                                      \n",
       "1   [father, dysfunctional, selfish, drags, kids, ...   \n",
       "2   [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "3                                   [bihday, majesty]   \n",
       "4                       [model, love, take, time, ur]   \n",
       "5                   [factsguide, society, motivation]   \n",
       "\n",
       "                                        tweet_stemmed  \\\n",
       "id                                                      \n",
       "1   [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "2   [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "3                                   [bihday, majesti]   \n",
       "4                       [model, love, take, time, ur]   \n",
       "5                         [factsguid, societi, motiv]   \n",
       "\n",
       "                                     tweet_lemmatized  \n",
       "id                                                     \n",
       "1   [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "2   [thanks, lyft, credit, use, cause, offer, whee...  \n",
       "3                                   [bihday, majesty]  \n",
       "4                       [model, love, take, time, ur]  \n",
       "5                   [factsguide, society, motivation]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c08f9f-1d39-4909-be1c-ef0aa079ed50",
   "metadata": {},
   "source": [
    "1.\n",
    "Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно. • Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df. • Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000. • Исключим стоп-слова с помощью stop_words='english'. • Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa4b215-6450-4c15-9005-a3a54751c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "811fdb1b-5f59-4086-92d9-9b2d6eba89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_stemmed'] = df['tweet_stemmed'].apply(lambda x: ' '.join(x))\n",
    "df['tweet_lemmatized'] = df['tweet_lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2824737a-5c3b-48ba-9a97-5f92cfd30e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_stem = CountVectorizer(max_df=0.9, max_features=1000, stop_words='english')\n",
    "cvec_lemm = CountVectorizer(max_df=0.9, max_features=1000, stop_words='english')\n",
    "\n",
    "bow_cv_stem = cvec_stem.fit_transform(df['tweet_stemmed'])\n",
    "bow_cv_lemm = cvec_lemm.fit_transform(df['tweet_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b03fda70-857b-4191-be0b-95d9f411171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_cv_stem = pd.DataFrame.sparse.from_spmatrix(bow_cv_stem, columns=cvec_stem.get_feature_names_out())\n",
    "bow_cv_lemm = pd.DataFrame.sparse.from_spmatrix(bow_cv_lemm, columns=cvec_lemm.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9fa407-3f1e-459f-bfa6-6803aebdb471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual  ad  adapt  ...  \\\n",
       "0    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "1    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "2    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "3    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "4    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "\n",
       "   yeah  year  yesterday  yo  yoga  york  young  youtub  yr  yummi  \n",
       "0     0     0          0   0     0     0      0       0   0      0  \n",
       "1     0     0          0   0     0     0      0       0   0      0  \n",
       "2     0     0          0   0     0     0      0       0   0      0  \n",
       "3     0     0          0   0     0     0      0       0   0      0  \n",
       "4     0     0          0   0     0     0      0       0   0      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_cv_stem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31696a42-d41a-4443-955d-0a8b5ee6f064",
   "metadata": {},
   "source": [
    "2. Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно. • Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df. • Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000. • Исключим стоп-слова с помощью stop_words='english'. • Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8370bbfe-e382-4b5b-9a9a-b139f05daa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca89bdef-30ff-4066-b69d-31de9a4a9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec_stem = TfidfVectorizer(max_df=0.9, max_features=1000, stop_words='english')\n",
    "tvec_lemm = TfidfVectorizer(max_df=0.9, max_features=1000, stop_words='english')\n",
    "\n",
    "bow_tv_stem = tvec_stem.fit_transform(df['tweet_stemmed'])\n",
    "bow_tv_lemm = tvec_lemm.fit_transform(df['tweet_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec59f6e2-e3c9-4f6e-92c2-5b3f91e2b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_tv_stem = pd.DataFrame.sparse.from_spmatrix(bow_tv_stem, columns=tvec_stem.get_feature_names_out())\n",
    "bow_tv_lemm = pd.DataFrame.sparse.from_spmatrix(bow_tv_lemm, columns=tvec_lemm.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5f53c0e-19de-4838-adcf-6d2378e6cdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>adventure</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  account  act  action  actor  actually  adapt  add  \\\n",
       "0   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "1   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "2   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "3   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "4   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "\n",
       "   adventure  ...  year  yes  yesterday   yo  yoga  york  young  youtube   yr  \\\n",
       "0        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "1        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "2        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "3        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "4        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "\n",
       "   yummy  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tv_lemm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd62187-c09c-497a-919c-80fc70dd99b8",
   "metadata": {},
   "source": [
    "3. Натренируем gensim.models.Word2Vec модель на наших данных. • Тренировать будем на токенизированных твитах combine_df['tweet_token'] • Установим следующие параметры: size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34. • Используем функцию train() с параметром total_examples равным длине combine_df['tweet_token'], количество epochs установим 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0243de8-4ed6-49a3-8c61-42d8ec405ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4667e23-e8a9-4d83-a5d7-88c084e40103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9199294, 11790760)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = word2vec.Word2Vec(df['tweet_token'], vector_size=200, window=5, min_count=2, sg=1, hs=0, negative=10, seed=34, workers=12)\n",
    "w2v.train(df['tweet_token'], total_examples=len(df), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca024898-9577-4c95-a2a4-03c51d00786a",
   "metadata": {},
   "source": [
    "4. Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово positive = \"dinner\", и модель вытащит из корпуса наиболее похожие слова c помощью функции most_similar. То же самое попробуем со словом \"trump\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8068ee52-36ad-49a9-916a-3c9231ffa09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bihdaydinner', 0.5568002462387085),\n",
       " ('cookout', 0.5498145818710327),\n",
       " ('bolognese', 0.5429084300994873),\n",
       " ('shawarma', 0.533199667930603),\n",
       " ('tacotuesday', 0.5323346257209778),\n",
       " ('hamburger', 0.5248205661773682),\n",
       " ('spaghetti', 0.522797167301178),\n",
       " ('waterloo', 0.5203596353530884),\n",
       " ('lastnight', 0.5141153931617737),\n",
       " ('lamb', 0.5126266479492188)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive='dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3afe46-919e-4975-83da-d8fd8129b62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 0.553157389163971),\n",
       " ('suppoer', 0.5381174683570862),\n",
       " ('trumptrain', 0.518347978591919),\n",
       " ('unfavorability', 0.5142640471458435),\n",
       " ('dumptrump', 0.5138635039329529),\n",
       " ('impeachment', 0.5101327896118164),\n",
       " ('donaldtrump', 0.5077646374702454),\n",
       " ('landslide', 0.5061473846435547),\n",
       " ('unstable', 0.5021856427192688),\n",
       " ('racists', 0.5021058320999146)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive='trump')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ce8b9-7275-4bee-8362-0634ec92e763",
   "metadata": {},
   "source": [
    "5. Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова). Давайте проверим векторное представление любого слова из нашего корпуса, например \"food\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95afabf2-5266-4ed4-aabf-d16761518ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47018722,  0.19807665,  0.51361686, -0.29027238, -0.08123974,\n",
       "       -0.15692584, -0.08659583, -0.07750746, -0.4769662 ,  0.07051579,\n",
       "        0.1553299 ,  0.664919  , -0.22626036,  0.432069  ,  0.41472256,\n",
       "       -0.46789902,  0.7674717 , -0.06155152, -0.19858138, -0.6964467 ,\n",
       "       -0.31180942,  0.02801759, -0.9658839 ,  0.0043061 ,  0.04740566,\n",
       "        0.7526299 , -1.2153084 , -0.4102754 ,  0.27914888, -0.19093771,\n",
       "        0.3142806 , -0.21757497, -0.15684263,  0.20230335, -1.1284038 ,\n",
       "       -0.3630038 ,  0.8751802 , -0.0635411 ,  0.14203243, -0.6639904 ,\n",
       "       -0.06075408, -0.45891157,  0.08201483,  0.7440632 ,  0.23648736,\n",
       "       -0.08807326, -0.4614497 , -0.4500064 , -0.37902114, -0.6233122 ,\n",
       "        0.04432596, -0.06405585,  0.02395995, -0.26261368, -0.3487461 ,\n",
       "       -0.9373858 , -0.2622752 ,  0.03379533, -0.34526038,  0.5597952 ,\n",
       "       -0.36933407,  0.16252871,  0.04648735, -0.44571123, -0.23196456,\n",
       "        0.212826  ,  0.901017  ,  0.66262233,  0.15821247,  0.21378878,\n",
       "        0.75001824, -0.8422707 ,  0.16060023,  0.47233826,  0.51123714,\n",
       "       -0.31728703, -0.4731496 , -0.54488814, -0.5452689 , -0.39940733,\n",
       "        0.06030555, -0.49315324, -0.27395692,  0.3825642 ,  0.31653845,\n",
       "        0.32369125,  0.06340362,  0.1774125 ,  0.73873705,  0.04955403,\n",
       "       -0.34988466, -0.12989947, -0.04667086,  0.30643862, -0.28146127,\n",
       "       -0.10081384, -0.5778028 , -0.14374869, -0.8229412 , -0.22469223,\n",
       "       -0.07559333, -0.58896893, -0.19207984,  0.36244154, -0.9816266 ,\n",
       "        0.27143764,  0.05985426, -0.7545296 , -0.14878245,  0.3206226 ,\n",
       "       -0.32402208,  0.24082376, -0.19886854, -0.28674343, -0.28326696,\n",
       "       -0.12394921,  0.33867678,  0.13487333, -0.09533616,  0.09413883,\n",
       "        0.12979685,  0.03333524, -0.46772715, -0.5252997 , -0.22075364,\n",
       "       -0.6265443 ,  0.63346994,  0.42354074,  0.557711  , -0.17315714,\n",
       "       -0.5093275 , -0.1453868 , -0.50456834,  0.5499433 ,  0.6241587 ,\n",
       "        0.47322482,  0.41153428, -0.53852105, -0.08569867,  0.65873843,\n",
       "        0.5333531 ,  0.55131114,  0.01737356,  0.14221215, -0.7399587 ,\n",
       "       -0.48101187, -0.16314863, -0.06213672,  0.31485566,  0.4981866 ,\n",
       "        0.1875908 ,  0.19651063,  0.18663299, -0.92220104, -0.09515753,\n",
       "       -0.5923514 ,  0.07929239, -0.40378773, -0.23010738,  0.76101565,\n",
       "        0.30459082,  0.07251049, -0.07102305, -0.44714198,  0.00368409,\n",
       "        0.17038652, -0.1578565 ,  0.85054106,  0.24438865,  0.11639286,\n",
       "       -0.55803984, -0.22656631,  0.09878872, -0.125979  , -0.7833867 ,\n",
       "       -0.11112032, -0.5215051 ,  0.2732927 , -0.37187794,  0.2132166 ,\n",
       "        0.5071607 ,  0.01180124,  0.7001186 , -0.72710514,  0.5102405 ,\n",
       "        0.28557235, -0.23616931,  0.43935266,  0.44741306,  0.101218  ,\n",
       "       -0.43053052, -0.23742954,  0.10927141, -0.20111048, -0.23656304,\n",
       "        0.15562767,  0.32950333, -0.25616223,  0.34963116,  0.46010074],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv['food']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb98db3-1095-438f-8d33-2b5ab7fc795c",
   "metadata": {},
   "source": [
    "6. Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите. Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. Теперь у нас есть 200 функций word2vec для наших данных. Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите. В цикле сделать: vec += model_w2v[word].reshape((1, size)) и поделить финальный вектор на количество слов в твите. На выходе должен получиться wordvec_df.shape = (49159, 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f2dca72-6358-479a-a846-4da1bcaad3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06b9888b-0d41-4d6e-883b-83cb21ebf8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vec(sentence, model):\n",
    "    sum_vec = np.zeros((1, model.vector_size))\n",
    "    i = 0\n",
    "    \n",
    "    for word in sentence['tweet_token']:\n",
    "        if word in model.wv:\n",
    "            sum_vec += model.wv[word].reshape(1, -1)\n",
    "            i += 1\n",
    "            \n",
    "    return np.squeeze(sum_vec / i) if i else np.squeeze(sum_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "640db497-7e59-42d2-9e21-6360d80e4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_vectors = df.apply(get_sentence_vec, axis=1, result_type='expand', model=w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ef42809-4f44-43de-a4c3-03f4faae960c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.029095</td>\n",
       "      <td>-0.120969</td>\n",
       "      <td>0.231957</td>\n",
       "      <td>-0.091578</td>\n",
       "      <td>-0.165193</td>\n",
       "      <td>0.020818</td>\n",
       "      <td>-0.205122</td>\n",
       "      <td>-0.267923</td>\n",
       "      <td>0.076370</td>\n",
       "      <td>0.009890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177879</td>\n",
       "      <td>-0.286595</td>\n",
       "      <td>0.073362</td>\n",
       "      <td>-0.089823</td>\n",
       "      <td>0.181927</td>\n",
       "      <td>0.201729</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>-0.221264</td>\n",
       "      <td>0.021728</td>\n",
       "      <td>-0.008503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.199682</td>\n",
       "      <td>-0.271347</td>\n",
       "      <td>0.055071</td>\n",
       "      <td>-0.146986</td>\n",
       "      <td>-0.233806</td>\n",
       "      <td>-0.091754</td>\n",
       "      <td>-0.227607</td>\n",
       "      <td>-0.293015</td>\n",
       "      <td>-0.026972</td>\n",
       "      <td>-0.085604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051128</td>\n",
       "      <td>-0.307307</td>\n",
       "      <td>0.171786</td>\n",
       "      <td>-0.079836</td>\n",
       "      <td>0.073574</td>\n",
       "      <td>0.165274</td>\n",
       "      <td>0.006235</td>\n",
       "      <td>-0.212228</td>\n",
       "      <td>0.189741</td>\n",
       "      <td>0.005495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.514459</td>\n",
       "      <td>-0.079656</td>\n",
       "      <td>0.091082</td>\n",
       "      <td>0.191382</td>\n",
       "      <td>-0.215843</td>\n",
       "      <td>0.155935</td>\n",
       "      <td>-0.257496</td>\n",
       "      <td>-0.469222</td>\n",
       "      <td>0.112373</td>\n",
       "      <td>-0.193821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204260</td>\n",
       "      <td>-0.758033</td>\n",
       "      <td>0.073523</td>\n",
       "      <td>-0.241930</td>\n",
       "      <td>0.012007</td>\n",
       "      <td>0.216654</td>\n",
       "      <td>-0.030310</td>\n",
       "      <td>0.102259</td>\n",
       "      <td>0.407475</td>\n",
       "      <td>0.385974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "id                                                                         \n",
       "1  -0.029095 -0.120969  0.231957 -0.091578 -0.165193  0.020818 -0.205122   \n",
       "2  -0.199682 -0.271347  0.055071 -0.146986 -0.233806 -0.091754 -0.227607   \n",
       "3  -0.514459 -0.079656  0.091082  0.191382 -0.215843  0.155935 -0.257496   \n",
       "\n",
       "         7         8         9    ...       190       191       192       193  \\\n",
       "id                                ...                                           \n",
       "1  -0.267923  0.076370  0.009890  ...  0.177879 -0.286595  0.073362 -0.089823   \n",
       "2  -0.293015 -0.026972 -0.085604  ...  0.051128 -0.307307  0.171786 -0.079836   \n",
       "3  -0.469222  0.112373 -0.193821  ...  0.204260 -0.758033  0.073523 -0.241930   \n",
       "\n",
       "         194       195       196       197       198       199  \n",
       "id                                                              \n",
       "1   0.181927  0.201729  0.000677 -0.221264  0.021728 -0.008503  \n",
       "2   0.073574  0.165274  0.006235 -0.212228  0.189741  0.005495  \n",
       "3   0.012007  0.216654 -0.030310  0.102259  0.407475  0.385974  \n",
       "\n",
       "[3 rows x 200 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_vectors.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bebf068-94bc-473f-9d02-d4cc68388650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_vectors.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
