{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc45dda-c226-415f-a0ac-95fba89fd750",
   "metadata": {},
   "source": [
    "## Практическое задание к уроку 5 по теме \"Part-of-Speech разметка, NER, извлечение отношений\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9e4e87-eca8-483e-8159-b4fa1296ab88",
   "metadata": {},
   "source": [
    "Задание 1. Написать теггер на данных с русским языком\n",
    "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
    "2. написать свой теггер как на занятии, попробовать разные\n",
    "векторайзеры, добавить знание не только букв но и слов  \n",
    "3. сравнить все реализованные методы, сделать выводы  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba280a-e65c-498c-9cef-a9f32296fba9",
   "metadata": {},
   "source": [
    "Загрузим библиотеки и датасеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964307df-de41-426a-b9e1-4fa6d29f8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger, DefaultTagger\n",
    "import pandas as pd\n",
    "import pyconll\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61964e7c-691c-4edb-bfe5-7c3b3223c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../Теория/Lesson_5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ec496c-d29b-40a3-8539-ed5bd0425c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file(path + 'dataset_ru/ru_syntagrus-ud-train-a.conllu')\n",
    "full_train_b = pyconll.load_from_file(path + 'dataset_ru/ru_syntagrus-ud-train-b.conllu')\n",
    "full_train_c = pyconll.load_from_file(path + 'dataset_ru/ru_syntagrus-ud-train-c.conllu')\n",
    "\n",
    "full_train.extend([*full_train_b, *full_train_c])\n",
    "\n",
    "full_test = pyconll.load_from_file(path + 'dataset_ru/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c5b401c-786d-4dcd-ba63-fdbc931e9740",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83cc0eb-a27a-41db-b3c9-6fe50d92e901",
   "metadata": {},
   "source": [
    "Протестируем различные тэггеры из библиотеки nltk и их комбинации.  \n",
    "Внесём получившиеся значения accuracy в таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "703ecefa-be88-4830-80db-d6e667096c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 341 ms, total: 1min 11s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "default_tagger = DefaultTagger('NOUN')\n",
    "possible_options = {\n",
    "    'Trigram': 'TrigramTagger(fdata_train)',\n",
    "    'Bigram': 'BigramTagger(fdata_train)',\n",
    "    'Unigram': 'UnigramTagger(fdata_train)',\n",
    "    'Default': 'default_tagger',\n",
    "    'Trigram + Default': 'TrigramTagger(fdata_train, backoff=default_tagger)',\n",
    "    'Trigram + Bigram': 'TrigramTagger(fdata_train, backoff=BigramTagger(fdata_train))',\n",
    "    'Trigram + Bigram + Default': 'TrigramTagger(fdata_train, backoff=BigramTagger(fdata_train, backoff=default_tagger))',\n",
    "    'Trigram + Unigram': 'TrigramTagger(fdata_train, backoff=UnigramTagger(fdata_train))',\n",
    "    'Trigram + Unigram + Default': 'TrigramTagger(fdata_train, backoff=UnigramTagger(fdata_train, backoff=default_tagger))',\n",
    "    'Trigram + Bigram + Unigram': 'TrigramTagger(fdata_train, backoff=BigramTagger(fdata_train, backoff=UnigramTagger(fdata_train)))',\n",
    "    'Trigram + Bigram + Unigram + Default': 'TrigramTagger(fdata_train, backoff=BigramTagger(fdata_train, backoff=UnigramTagger(fdata_train, backoff=default_tagger)))',\n",
    "    'Bigram + Default': 'BigramTagger(fdata_train, backoff=default_tagger)',\n",
    "    'Bigram + Unigram': 'BigramTagger(fdata_train, backoff=UnigramTagger(fdata_train))',\n",
    "    'Bigram + Unigram + Default': 'BigramTagger(fdata_train, backoff=UnigramTagger(fdata_train, backoff=default_tagger))',\n",
    "    'Unigram + Default': 'UnigramTagger(fdata_train, backoff=default_tagger)'    \n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for name, tagger in possible_options.items():\n",
    "    scores.append((round(eval(tagger).accuracy(fdata_test), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cda28776-a20c-4b24-821c-fdaa7b8c9865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Trigram + Unigram + Default</th>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Bigram + Unigram + Default</th>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram + Unigram + Default</th>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unigram + Default</th>\n",
       "      <td>0.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram + Unigram</th>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Unigram</th>\n",
       "      <td>0.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Bigram + Unigram</th>\n",
       "      <td>0.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unigram</th>\n",
       "      <td>0.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Bigram + Default</th>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram + Default</th>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Default</th>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram</th>\n",
       "      <td>0.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Bigram</th>\n",
       "      <td>0.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram</th>\n",
       "      <td>0.407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Default</th>\n",
       "      <td>0.236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Score\n",
       "Trigram + Unigram + Default           0.912\n",
       "Trigram + Bigram + Unigram + Default  0.912\n",
       "Bigram + Unigram + Default            0.912\n",
       "Unigram + Default                     0.906\n",
       "Bigram + Unigram                      0.884\n",
       "Trigram + Unigram                     0.883\n",
       "Trigram + Bigram + Unigram            0.883\n",
       "Unigram                               0.878\n",
       "Trigram + Bigram + Default            0.861\n",
       "Bigram + Default                      0.861\n",
       "Trigram + Default                     0.791\n",
       "Bigram                                0.710\n",
       "Trigram + Bigram                      0.703\n",
       "Trigram                               0.407\n",
       "Default                               0.236"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = pd.DataFrame(scores, index=possible_options.keys(), columns=['Score']).sort_values('Score', ascending=False)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1257953d-d61d-46a0-9b8a-82ab93a26114",
   "metadata": {},
   "source": [
    "Лучшим среди одиночных тэггеров оказался Unigram, поэтому все  \n",
    "лучшие комбинации были с ним в составе. Добавление в качестве  \n",
    "последнего тэггера дефолтный даёт прирост всем комбинациям, что  \n",
    "довольно логично, ведь мы вместо None на незнакомых словах ставим  \n",
    "тэг существительного, и иногда попадаем."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2051bf7-ab6c-494a-9466-1bb413f3a609",
   "metadata": {},
   "source": [
    "Напишем свой тэггер. Попробую сделать тэггер, который на  \n",
    "тренировочном датасете будет запоминать, какой тэг стоял  \n",
    "до и после текущего слова. Тэг текущего слова будет выбираться  \n",
    "равным либо тэгу предыдущего, либо тэгу следующего, смотря каких  \n",
    "тэгов было больше на трейне. В данном случае тэг текущего слова  \n",
    "никак не будет учитываться, и, скорее всего, такой тэггер покажет  \n",
    "плохой результат, но мы потренируемся в написании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e087cd-12b2-47e1-becb-33e0b16b8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf1819d-9aff-4643-9da9-e27d90e19675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTagger(nltk.tag.SequentialBackoffTagger):\n",
    "    def __init__(self, train, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Словари, где ключами являются текущие слова, а значениями - словари, \n",
    "        # где ключи - тэги предыдущего (следующего) слова, а значения - количество\n",
    "        # этих тэгов:\n",
    "        self.prev_values_dict = {} \n",
    "        self.next_values_dict = {} \n",
    "                                   \n",
    "        for sent in train:\n",
    "            words = [pair[0] for pair in sent]\n",
    "            tags = [pair[1] for pair in sent]\n",
    "            for i, word in enumerate(words):\n",
    "                if word is None:\n",
    "                    continue\n",
    "                word = word.lower()\n",
    "                \n",
    "                # Если текущего слова нет в словаре, то создаём  для него defaultdict:\n",
    "                if i != 0:\n",
    "                    if word not in self.prev_values_dict:\n",
    "                        self.prev_values_dict[word] = defaultdict(int)\n",
    "                    self.prev_values_dict[word][tags[i-1]] += 1\n",
    "                    \n",
    "                if i != len(words) - 1:\n",
    "                    if word not in self.next_values_dict:\n",
    "                        self.next_values_dict[word] = defaultdict(int)\n",
    "                    self.next_values_dict[word][tags[i+1]] += 1\n",
    "    \n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        word = tokens[index]\n",
    "        if word is None:\n",
    "            return None\n",
    "        word = word.lower()\n",
    "        next_count = 0\n",
    "        prev_count = 0\n",
    "        \n",
    "        # Находим наиболее популярные тэги предыдущего и следующего слова для текущего слова\n",
    "        if word in self.prev_values_dict:\n",
    "            prev_popular, prev_count = sorted(self.prev_values_dict[word].items(), key=lambda x: x[1], reverse=True)[0]\n",
    "        if word in self.next_values_dict:\n",
    "            next_popular, next_count = sorted(self.next_values_dict[word].items(), key=lambda x: x[1], reverse=True)[0]\n",
    "        \n",
    "        # Если слова нет ни в одном словаре, то возвращаем None\n",
    "        if (prev_count == 0) and (next_count == 0):\n",
    "            return None\n",
    "        \n",
    "        # Возвращаем наиболее популярный тэг из тэгов следующего (предыдущего) слова\n",
    "        if prev_count >= next_count:\n",
    "            return prev_popular\n",
    "        elif prev_count < next_count:\n",
    "            return next_popular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f8048-3778-4bda-87d0-e8f023aab753",
   "metadata": {},
   "source": [
    "Обучим тэггер и проверим его точность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ae8d20c-7cf8-4abb-8b9a-65f98569edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tagger = MyTagger(fdata_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fa191ed-71ac-45e2-a14b-45ce13ca4acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06064847971873169"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tagger.accuracy(fdata_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c45c68-2370-4619-84c5-f89df89a4075",
   "metadata": {},
   "source": [
    "Как и ожидалось, тэггер оказался плох, намного хуже даже дефолтного.  \n",
    "Теперь напишем более простой тэггер, который является аналогом Unigram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "202ac88f-3ce1-4a7f-9051-d2413800f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyUniTagger(nltk.tag.SequentialBackoffTagger):\n",
    "    def __init__(self, train, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Снова словарь словарей, но теперь в ключах defaultdict  \n",
    "        # будут тэги текущего слова\n",
    "        self.dictionary = {}\n",
    "        \n",
    "        for sent in train:\n",
    "            for word, tag in sent:\n",
    "                if word is None:\n",
    "                    continue\n",
    "                    \n",
    "                word = word.lower()\n",
    "                if word not in self.dictionary:\n",
    "                    self.dictionary[word] = defaultdict(int)\n",
    "                self.dictionary[word][tag] += 1\n",
    "            \n",
    "    \n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        word = tokens[index]\n",
    "        if word is None:\n",
    "            return None\n",
    "        word = word.lower()\n",
    "        \n",
    "        # Возвращаем наиболее популярный тэг текущего слова\n",
    "        if word in self.dictionary:\n",
    "            return sorted(self.dictionary[word].items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d384b-36b7-4e3c-97a0-89154aea7180",
   "metadata": {},
   "source": [
    "Оценим этот тэггер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fabf107f-ec10-4c42-886e-39865759e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_unitagger = MyUniTagger(fdata_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30fad166-5c2c-4c50-9ed1-211ec6d27f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8829611302819194"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_unitagger.accuracy(fdata_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9782bb-ab94-4d25-b1fd-5e4f6273db1c",
   "metadata": {},
   "source": [
    "Добавим новые данные в таблицу метрик:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3071e03-0f90-4d49-ab65-12770e4fd47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Trigram + Unigram + Default</th>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Bigram + Unigram + Default</th>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram + Unigram + Default</th>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MyUniTagger + Default</th>\n",
       "      <td>0.908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unigram + Default</th>\n",
       "      <td>0.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram + Unigram</th>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Unigram</th>\n",
       "      <td>0.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Bigram + Unigram</th>\n",
       "      <td>0.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MyUniTagger</th>\n",
       "      <td>0.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unigram</th>\n",
       "      <td>0.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Bigram + Default</th>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram + Default</th>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Default</th>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram</th>\n",
       "      <td>0.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Bigram</th>\n",
       "      <td>0.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram</th>\n",
       "      <td>0.407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Default</th>\n",
       "      <td>0.236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MyTagger + Default</th>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MyTagger</th>\n",
       "      <td>0.061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Score\n",
       "Trigram + Unigram + Default           0.912\n",
       "Trigram + Bigram + Unigram + Default  0.912\n",
       "Bigram + Unigram + Default            0.912\n",
       "MyUniTagger + Default                 0.908\n",
       "Unigram + Default                     0.906\n",
       "Bigram + Unigram                      0.884\n",
       "Trigram + Unigram                     0.883\n",
       "Trigram + Bigram + Unigram            0.883\n",
       "MyUniTagger                           0.883\n",
       "Unigram                               0.878\n",
       "Trigram + Bigram + Default            0.861\n",
       "Bigram + Default                      0.861\n",
       "Trigram + Default                     0.791\n",
       "Bigram                                0.710\n",
       "Trigram + Bigram                      0.703\n",
       "Trigram                               0.407\n",
       "Default                               0.236\n",
       "MyTagger + Default                    0.087\n",
       "MyTagger                              0.061"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_mytaggers = pd.concat((comparison, pd.DataFrame(round(my_tagger.accuracy(fdata_test), 3), \n",
    "                                                       index=['MyTagger'], columns=['Score'])), axis=0)\n",
    "comparison_mytaggers = pd.concat((comparison_mytaggers, pd.DataFrame(round(my_unitagger.accuracy(fdata_test), 3), \n",
    "                                                       index=['MyUniTagger'], columns=['Score'])), axis=0)\n",
    "comparison_mytaggers = pd.concat((comparison_mytaggers, pd.DataFrame(round(MyTagger(fdata_train, backoff=default_tagger)\\\n",
    "                                                             .accuracy(fdata_test), 3), \n",
    "                                                       index=['MyTagger + Default'], columns=['Score'])), axis=0)\n",
    "comparison_mytaggers = pd.concat((comparison_mytaggers, pd.DataFrame(round(MyUniTagger(fdata_train, backoff=default_tagger)\\\n",
    "                                                             .accuracy(fdata_test), 3), \n",
    "                                                       index=['MyUniTagger + Default'], columns=['Score'])), axis=0)\n",
    "\n",
    "comparison_mytaggers.sort_values('Score', ascending=False, inplace=True)\n",
    "comparison_mytaggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042b7b7-9f43-4d98-8348-fc1ec00cfcff",
   "metadata": {},
   "source": [
    "Наш тэггер оказался чуть лучше Unigram тэггера nltk, а его комбинация  \n",
    "с дефолтным показала второй результат."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddacd30f-965b-468d-9e07-641878c24678",
   "metadata": {},
   "source": [
    "Теперь обучим логистическую регрессию на мультиклассовую  \n",
    "классификацию, чтобы предсказывать тэги. Подготовку данных  \n",
    "будем производить с помощью встроенных в sklearn векторайзеров.  \n",
    "Попробуем варианты с векторизацией символов и их комбинаций, а затем  \n",
    "слов и их комбинаций.  \n",
    "Сделаем предобработку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "701140d6-286b-48bb-b486-0da9adec1a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "y_train = []\n",
    "\n",
    "# В трейн не будем записывать пропущенные значения токенов или тэгов\n",
    "for sent in fdata_train:\n",
    "    for word, tag in sent:\n",
    "        if (word is None) or (tag is None):\n",
    "            continue\n",
    "        train_tok.append(word)\n",
    "        y_train.append(tag)\n",
    "        \n",
    "test_tok = []\n",
    "y_test = []\n",
    "\n",
    "# В тест будем записывать всё для более честного сравнения с тэггерами\n",
    "for sent in fdata_test:\n",
    "    for word, tag in sent:\n",
    "        if word is None:\n",
    "            test_tok.append('NO_WORD')\n",
    "        else:\n",
    "            test_tok.append(word)\n",
    "        if tag is None:\n",
    "            y_test.append('NO_TAG')\n",
    "        else:\n",
    "            y_test.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ef1c3-f97f-4cff-ad80-3fca4db3df32",
   "metadata": {},
   "source": [
    "Посмотрим на распределение классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7728e94f-5625-4f46-984e-ac584771707d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOUN      0.235940\n",
       "PUNCT     0.190025\n",
       "VERB      0.111400\n",
       "ADJ       0.098333\n",
       "ADP       0.089309\n",
       "ADV       0.050674\n",
       "PRON      0.048467\n",
       "CCONJ     0.036929\n",
       "PROPN     0.035634\n",
       "PART      0.033368\n",
       "DET       0.027769\n",
       "SCONJ     0.018654\n",
       "NUM       0.011290\n",
       "AUX       0.009050\n",
       "NO_TAG    0.001725\n",
       "X         0.000872\n",
       "SYM       0.000404\n",
       "INTJ      0.000156\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0921bd46-957c-4c67-9c14-f2c672e0efa2",
   "metadata": {},
   "source": [
    "Может быть, в данном случае метрика accuracy и не лучший выбор, но,  \n",
    "во-первых, на такой метрике мы проверяли тэггеры. Во-вторых, самый  \n",
    "популярный класс имеет долю 23%, а это далеко от метрики лучших тэггеров.  \n",
    "Так что случайная модель всё равно не покажет хороший результат."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da12fca-578a-413d-afaf-1a83f1f721b2",
   "metadata": {},
   "source": [
    "Сначала применим векторизацию по символам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4b29673-dc9e-4c05-87ae-cb84703ca826",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_char = TfidfVectorizer(ngram_range=(1, 6), analyzer='char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad357863-277e-4256-98b9-bf2eaced38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer_char.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d92d8e1-773d-4b19-a92a-ee2c8b133046",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer_char.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "797fc890-9a87-4b19-a283-1db820d27e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1204640, 304810), (153590, 304810))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f13e405-6b02-4028-b1f9-e2856a5a1369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.7 s, sys: 180 ms, total: 48.9 s\n",
      "Wall time: 48.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=29, solver=&#x27;sag&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=29, solver=&#x27;sag&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=29, solver='sag')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_char = LogisticRegression(random_state=29, solver='sag')\n",
    "lr_char.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "791c97f1-9cb5-4397-8206-2337321db6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_char.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "370bfb81-4c76-4965-8674-61a3572e20cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.944885734748356"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_char_score = accuracy_score(y_test, pred)\n",
    "lr_char_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187e05b-74ea-4ba9-ada7-4406c775ed20",
   "metadata": {},
   "source": [
    "Неплохо! Помимо Tfidf были опробованы HashingVectorizer и  \n",
    "CountVectorizer. Результат Hashing оказался хуже, чем Tfidf.  \n",
    "Результат CountVectorizer получился выше примерно на 0,5%, но  \n",
    "при этом модель обучалась в несколько раз дольше. Для занятия  \n",
    "первой строчки в таблице хватило и этого варианта."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4fc31-33f5-4c0d-8656-3dac855dba7a",
   "metadata": {},
   "source": [
    "Применим векторизацию по словам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89e3fcab-e175-47b3-a464-5fd17545f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_word = TfidfVectorizer(ngram_range=(1, 2), analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbafbaf0-a41f-4e40-946a-f176b54c8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer_word.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82f6c862-c4e5-48c6-ad54-8c32c9462dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer_word.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1df34729-6868-469b-b971-d6558533af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.6 s, sys: 152 ms, total: 22.7 s\n",
      "Wall time: 22.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=29, solver=&#x27;sag&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=29, solver=&#x27;sag&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=29, solver='sag')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_word = LogisticRegression(random_state=29, solver='sag')\n",
    "lr_word.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96b9eee7-1f1c-4385-a626-c09080f13d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_word.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "076e550f-f6f0-4af8-986b-a4448472a0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7619636695097337"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_word_score = accuracy_score(y_test, pred)\n",
    "lr_word_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630bd0c3-e0b1-4cdd-b43a-ab7488bd2b0b",
   "metadata": {},
   "source": [
    "Здесь Tfidf и CountVectorizer получили примерно одинаковый результат.  \n",
    "HashingVectorizer хуже примерно на 0,5%.  \n",
    "Внесём все данные в таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21d97b86-b9d8-47d6-be11-a73dc3c093a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Char Tfidf + Logistic Regression</th>\n",
       "      <td>0.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Unigram + Default</th>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram + Unigram + Default</th>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Bigram + Unigram + Default</th>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MyUniTagger + Default</th>\n",
       "      <td>0.908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unigram + Default</th>\n",
       "      <td>0.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram + Unigram</th>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Unigram</th>\n",
       "      <td>0.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Bigram + Unigram</th>\n",
       "      <td>0.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MyUniTagger</th>\n",
       "      <td>0.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unigram</th>\n",
       "      <td>0.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Bigram + Default</th>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram + Default</th>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Default</th>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word Tfidf + Logistic Regression</th>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram</th>\n",
       "      <td>0.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram + Bigram</th>\n",
       "      <td>0.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram</th>\n",
       "      <td>0.407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Default</th>\n",
       "      <td>0.236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MyTagger + Default</th>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MyTagger</th>\n",
       "      <td>0.061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Score\n",
       "Char Tfidf + Logistic Regression      0.945\n",
       "Trigram + Unigram + Default           0.912\n",
       "Bigram + Unigram + Default            0.912\n",
       "Trigram + Bigram + Unigram + Default  0.912\n",
       "MyUniTagger + Default                 0.908\n",
       "Unigram + Default                     0.906\n",
       "Bigram + Unigram                      0.884\n",
       "Trigram + Unigram                     0.883\n",
       "Trigram + Bigram + Unigram            0.883\n",
       "MyUniTagger                           0.883\n",
       "Unigram                               0.878\n",
       "Trigram + Bigram + Default            0.861\n",
       "Bigram + Default                      0.861\n",
       "Trigram + Default                     0.791\n",
       "Word Tfidf + Logistic Regression      0.762\n",
       "Bigram                                0.710\n",
       "Trigram + Bigram                      0.703\n",
       "Trigram                               0.407\n",
       "Default                               0.236\n",
       "MyTagger + Default                    0.087\n",
       "MyTagger                              0.061"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_mytaggers_vectorizers = pd.concat((comparison_mytaggers, \n",
    "                                              pd.DataFrame(round(lr_char_score, 3), index=['Char Tfidf + Logistic Regression'], \n",
    "                                                                                    columns=['Score'])), axis=0)\n",
    "comparison_mytaggers_vectorizers = pd.concat((comparison_mytaggers_vectorizers, \n",
    "                                              pd.DataFrame(round(lr_word_score, 3), index=['Word Tfidf + Logistic Regression'], \n",
    "                                                                                    columns=['Score'])), axis=0)\n",
    "comparison_mytaggers_vectorizers.sort_values('Score', ascending=False, inplace=True)\n",
    "comparison_mytaggers_vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec42de9f-8985-419b-a3ac-d8c118f7be85",
   "metadata": {},
   "source": [
    "С большим отрывом лидирует логистическая регрессия с Tfidf векторизацией  \n",
    "по символам. Векторизация по словам оказалась значительно хуже, и это объяснимо:  \n",
    "ведь о части речи может говорить какая-то часть слова, например, окончание, а не  \n",
    "всё слово целиком."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
