{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f303e5-081f-466a-be47-96537b35141e",
   "metadata": {},
   "source": [
    "## Практическое задание к уроку 3 по теме \"Embedding word2vec fasttext\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc59563-ac52-4578-823d-c74c114f09f0",
   "metadata": {},
   "source": [
    "Задача поиск похожих по эмбеддингам  \n",
    "\n",
    "Скачиваем датасет (источник): положительные, отрицательные.  \n",
    "\n",
    "или можно через ноутбук  \n",
    "  \n",
    "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv  \n",
    "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv  \n",
    "\n",
    "что надо сделать  \n",
    "1. объединить в одну выборку  \n",
    "2. на основе word2vec/fasttext/glove/слоя Embedding реализовать метод поиска ближайших твитов  \n",
    "(на вход метода должен приходить запрос (какой-то твит, вопрос) и количество вариантов вывода к примеру 5-ть, ваш метод должен возвращать 5-ть ближайших твитов к этому запросу)  \n",
    "3. Проверить насколько хорошо работают подходы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73b801-1609-4581-bce1-064a089fe8be",
   "metadata": {},
   "source": [
    "Загрузим необходимые библиотеки и датасеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936c794e-cb8f-435e-9627-f818a655bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import annoy\n",
    "from gensim.models import FastText\n",
    "from multiprocessing import Pool\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re\n",
    "from stop_words import get_stop_words\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "import urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d24baa44-cdee-4ff2-8a2b-7e36b4cef841",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 29\n",
    "NUM_THREADS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d72f40-617b-4301-a083-7222609dbfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226834, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos = pd.read_csv('../../Теория/Lesson_2/positive.csv', sep=';', header=None, usecols=[3], names=['tweet'])\n",
    "df_neg = pd.read_csv('../../Теория/Lesson_2/negative.csv', sep=';', header=None, usecols=[3], names=['tweet'])\n",
    "df = pd.concat((df_pos, df_neg), axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691aee7d-b5fc-4d83-8da7-b122448f2eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  @first_timee хоть я и школота, но поверь, у на...\n",
       "1  Да, все-таки он немного похож на него. Но мой ...\n",
       "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...\n",
       "3  RT @digger2912: \"Кто то в углу сидит и погибае...\n",
       "4  @irina_dyshkant Вот что значит страшилка :D\\nН..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a711d8-8cf8-45e9-9491-ae591f685814",
   "metadata": {},
   "source": [
    "Загрузим стопслова, добавим туда часто встречающееся слово rt, которое субъективно  \n",
    "не несёт смысла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86debb8-6c59-4efe-a176-b1e73fd2fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = get_stop_words('russian')\n",
    "sw.append('rt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e26d361-606e-47ce-a333-6fb27be5f2b1",
   "metadata": {},
   "source": [
    "Будем чистить от пунктуации, но оставим знаки, которые могут  \n",
    "помешать работе нашего токенайзера, заточенного на работу с твитами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83993d6a-ef56-44c0-9a24-5d87ee422a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = re.sub('[@_]', '', punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed469813-7e9e-4801-8a72-730058bb693c",
   "metadata": {},
   "source": [
    "Будем искать и убирать ссылки, так как они могут помешать работе  \n",
    "модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "910c5816-7014-4f60-b396-2de9bb2953ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_extractor = urlextract.URLExtract()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25a3ce-35e6-4609-9d30-40ab7d59aab0",
   "metadata": {},
   "source": [
    "Берём nltk токенайзер по твитам, лемматизацию будем проводить  \n",
    "с помощью pymorphy2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78bc0930-793f-4605-8cc3-40dc85a95446",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(reduce_len=True, strip_handles=True)\n",
    "lemmatizer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb787d-fd90-4ce4-925b-e8aaea56c6d8",
   "metadata": {},
   "source": [
    "Напишем функцию для обработки наших твитов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bf41791-50b5-441c-a21b-3caf6c462d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \n",
    "    # Чистим ссылки\n",
    "    urls = url_extractor.find_urls(text, only_unique=True)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, ' ')\n",
    "        \n",
    "    # Убираем смайлик с буквой, который может остаться\n",
    "    # после чистки от пунктуации, и приводим к нижнему\n",
    "    # регистру\n",
    "    text = re.sub(':D+', ' ', text).lower()\n",
    "    \n",
    "    # Чистим пунктуацию\n",
    "    text = re.sub(fr'[{punct}]+', ' ', text)\n",
    "    \n",
    "    # Убираем всё, не являющееся набором букв, в т.ч. цифры\n",
    "    text = ' '.join(word for word in text.split() if word.isalpha())\n",
    "    \n",
    "    # Токенизация\n",
    "    text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Лемматизация\n",
    "    text = [lemmatizer.parse(word)[0].normal_form for word in text]\n",
    "    \n",
    "    # Убираем стопслова\n",
    "    text = [word for word in text if word not in sw]\n",
    "    \n",
    "    # Возвращаем обработанный текст\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcdb89-58c4-483c-bd66-e8608025b621",
   "metadata": {},
   "source": [
    "Обрабатываем текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e795ffc8-dfb9-495b-974a-07d5a438ac5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 226834/226834 [01:05<00:00, 3439.22it/s]\n"
     ]
    }
   ],
   "source": [
    "with Pool(NUM_THREADS) as p:\n",
    "    df['processed_tweet'] = tqdm(p.imap(process_text, df['tweet']), total=len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51eb1ec8-ca0b-42b6-8c86-b3e02a77364d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>[школотый, поверь, самый, общество, профилиров...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>[таки, похожий, мальчик, равно, хороший]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>[идиотка, испугаться]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>[угол, сидеть, погибать, голод, порция, взять,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>[страшилка, блин, посмотреть, часть, создаться...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  @first_timee хоть я и школота, но поверь, у на...   \n",
       "1  Да, все-таки он немного похож на него. Но мой ...   \n",
       "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...   \n",
       "3  RT @digger2912: \"Кто то в углу сидит и погибае...   \n",
       "4  @irina_dyshkant Вот что значит страшилка :D\\nН...   \n",
       "\n",
       "                                     processed_tweet  \n",
       "0  [школотый, поверь, самый, общество, профилиров...  \n",
       "1           [таки, похожий, мальчик, равно, хороший]  \n",
       "2                              [идиотка, испугаться]  \n",
       "3  [угол, сидеть, погибать, голод, порция, взять,...  \n",
       "4  [страшилка, блин, посмотреть, часть, создаться...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294cdc4-7c0f-40d9-bee9-35f698a59070",
   "metadata": {},
   "source": [
    "Обучаем модель FastText, т.к. в твитах большая вероятность появления  \n",
    "опечаток, а эта модель обучается также и на n-граммах и имеет некоторую  \n",
    "устойчивость к опечаткам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c26f9555-48a0-4184-a191-64c078b57e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 16s, sys: 1.13 s, total: 8min 17s\n",
      "Wall time: 54.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ft_model = FastText(df['processed_tweet'], vector_size=300, window=3, min_count=2, \n",
    "                    seed=RANDOM_STATE, workers=NUM_THREADS, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba991cb-9fc0-4048-881a-350b2815041b",
   "metadata": {},
   "source": [
    "Напишем функцию для получения вектора твитов через векторы слов.  \n",
    "Будем просто находить среднее всех векторов слов твита, имеющихся  \n",
    "в словаре модели, либо возвращать нулевой вектор, если ни одного  \n",
    "слова нет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae083e97-57a8-4353-89ab-876e66da1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence, model):\n",
    "    counter = 0\n",
    "    vec = np.zeros(model.vector_size)\n",
    "    for word in sentence:\n",
    "        if word in model.wv:\n",
    "            vec += model.wv[word]\n",
    "            counter += 1\n",
    "    return vec / counter if counter else vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ab134-0924-464b-9df5-ab9e73370653",
   "metadata": {},
   "source": [
    "На основании векторов твитов построим деревья annoy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08295c58-874e-4368-acf5-b139bd3935ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.6 s, sys: 207 ms, total: 29.8 s\n",
      "Wall time: 10.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ft_index = annoy.AnnoyIndex(ft_model.vector_size, 'angular')\n",
    "\n",
    "for i, sentence in enumerate(df['processed_tweet']):\n",
    "    ft_vec = get_sentence_vector(sentence, ft_model)\n",
    "    ft_index.add_item(i, ft_vec)\n",
    "\n",
    "ft_index.build(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde015a1-6adf-4f3e-b1f5-5bac80b02a64",
   "metadata": {},
   "source": [
    "Находить близкие твиты будем с помощью annoy, напишем функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "724635fe-76b5-47c5-b340-f33e0cd62175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tweets(tweet, model, k=5):\n",
    "    tweet = process_text(tweet)\n",
    "    tweet_vec = get_sentence_vector(tweet, model)\n",
    "    idxs = ft_index.get_nns_by_vector(tweet_vec, k)\n",
    "    return df['tweet'].iloc[idxs].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4c2b7-ef1b-41a7-982a-a466fb98dab4",
   "metadata": {},
   "source": [
    "Напишем произвольные предложения и проверим на них работу модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e7181b6-96e4-4e88-ac0e-c49e1cc91408",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['Я просто не знаю, что дальше делать!',\n",
    "         'Сколько нужно денег для счастья?',\n",
    "         'Почему ты мне не звонишь?',\n",
    "         'Я сегодня купил чёрствый хлеб!',\n",
    "         'Делаешь плохо - получается плохо! Делаешь хорошо - получается хорошо!'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "118e7243-995f-4ae0-a7d7-1aac1cccfa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "1 Я просто не знаю, что дальше делать!\n",
      "**************************************************\n",
      "['RT @SemenSemin: Теперь будем знать, как это делать))\\nhttps://t.co/iRZJExZ9kq'\n",
      " 'ну вы знаете , что делать :))) http://t.co/PjRasgg62q'\n",
      " 'Спасибо тебе, @Kovalskiyyy, не знаю, чтобы я без тебя делала бы:*'\n",
      " '@kamilla_kub Только не знаю делают на мой или нет(('\n",
      " 'не знаю, что теперь делать, а было все так хорошо(']\n",
      "**************************************************\n",
      "\n",
      "2 Сколько нужно денег для счастья?\n",
      "**************************************************\n",
      "['@pechNnik @Pensionnik Не в деньгах счастье=('\n",
      " 'RT @385842387: не в деньгах счастье, а в их количестве!:)'\n",
      " '@gurodynawyr, Если ваше счастье не в деньгах, шлите их мне)'\n",
      " '@kolekcijaEDA камень - это разве подарок?)не в деньгах счастье)'\n",
      " 'Почему у всех есть деньги на веселье, а у меня нет?:(']\n",
      "**************************************************\n",
      "\n",
      "3 Почему ты мне не звонишь?\n",
      "**************************************************\n",
      "['RT @BloodStream69: @kiss_off_death а я же ей не в 6 буду звонить:D'\n",
      " '@Mashusha0404 а мы тебе звонили сегодня все('\n",
      " '@lena_ermolaeva ты мне даже не звонишь (('\n",
      " 'RT @cerdce2012013: Что-то @Yuuupiiiiii не звонит:('\n",
      " '@a_brex @cheese_is @daikiri_ice я и звонил(']\n",
      "**************************************************\n",
      "\n",
      "4 Я сегодня купил чёрствый хлеб!\n",
      "**************************************************\n",
      "['RT @tasteYOGURT: @Bloody_SAW82 А Я СЕБЕ СУХАРИКОВ КУПИЛ :D'\n",
      " '@SkyBOgach ))))))))))) купи себе пирожок!)))))))))))'\n",
      " 'Я давно хотела...и только сегодня купила Аромо лампу =)'\n",
      " 'купи мне бентли, купи купи))))) http://t.co/Uo1o8ITXmN'\n",
      " '@innochka2013 Очень мало, а я еще не все купила(']\n",
      "**************************************************\n",
      "\n",
      "5 Делаешь плохо - получается плохо! Делаешь хорошо - получается хорошо!\n",
      "**************************************************\n",
      "['Дима умеет делать капельку, так то круто, а у меня плохо получается :-('\n",
      " '@mon_liz1 Всё равно они очень плохо делают!:( А ты засыпай, Лизонька.'\n",
      " 'Сегодня игнорю СруденкАс,но у меня это ни хрена не получается делать('\n",
      " 'Все очень плохо.... Хочеться плакать економетрия что ты делаешь?('\n",
      " 'RT @NastasjaKostiuk: умоляю вас, не делайте другим плохого, вам же ведь вернется (']\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('*' * 50)\n",
    "for i, text in enumerate(texts):\n",
    "    print(f'{i+1} {text}')\n",
    "    print('*' * 50)\n",
    "    print(get_similar_tweets(text, ft_model, k=5))\n",
    "    print('*' * 50, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b8606-958f-4bb7-89d6-a4e35fceb27f",
   "metadata": {},
   "source": [
    "В целом, модель улавливает похожие тексты, но, к примеру, в 4 примере модель  \n",
    "\"поняла\", что такое \"покупка\", но \"не понимает\", что такое \"чёрствый хлеб\".  \n",
    "Скорее всего из-за того, что не встречала этого на обучении."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
